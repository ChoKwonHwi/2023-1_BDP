{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Conv2D, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448b242",
   "metadata": {},
   "source": [
    "# 1. subjects ID Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/sub_id3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489505d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사람_ID를 범주형 변수로 변환\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['ID_encoded'] = label_encoder.fit_transform(df['sub_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac46aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer 생성\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "print(tf.__version__)\n",
    "\n",
    "n_unique_ids = df['ID_encoded'].nunique()\n",
    "embedding_dim = min(n_unique_ids // 2, 50)  # 임베딩 차원 설정. 보통은 고유 ID의 수의 절반 혹은 50을 선택\n",
    "\n",
    "embedding_layer = Embedding(input_dim=n_unique_ids, \n",
    "                            output_dim=embedding_dim, \n",
    "                            input_length=1, \n",
    "                            name='ID_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1494aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 레이어를 통과하여 벡터 생성 후, Flatten 적용\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# 데이터를 모델에 넣을 수 있는 형태로 변환\n",
    "input_data = df['ID_encoded'].values.reshape(-1, 1)\n",
    "\n",
    "# 임베딩 레이어를 통과\n",
    "embedded_data = embedding_layer(input_data)\n",
    "\n",
    "# Flatten the output to enable concatenation\n",
    "sub_id_input = Flatten()(embedded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e322cd",
   "metadata": {},
   "source": [
    "# 2. video_type_OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/video_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(df['video_ID'].values.reshape(-1, 1))\n",
    "\n",
    "# Convert the result back to a dataframe\n",
    "df_encoded = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.categories_[0])\n",
    "\n",
    "# Convert df_encoded to a suitable input for Keras\n",
    "video_id_input = df_encoded.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66034673",
   "metadata": {},
   "source": [
    "# 3. EEG Signal Denoising AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd76e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages & libraries\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import cwt, ricker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from math import ceil\n",
    "import gc\n",
    "\n",
    "eeg_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG'\n",
    "sub_id_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/sub_id2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3ea2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data for sub_id 001\n",
      "Processed data for sub_id 005\n",
      "Processed data for sub_id 006\n",
      "Processed data for sub_id 009\n",
      "Processed data for sub_id 010\n",
      "Processed data for sub_id 011\n",
      "Processed data for sub_id 012\n",
      "Processed data for sub_id 013\n",
      "Processed data for sub_id 015\n",
      "Processed data for sub_id 021\n",
      "Processed data for sub_id 022\n",
      "Processed data for sub_id 023\n",
      "Processed data for sub_id 024\n",
      "Processed data for sub_id 026\n",
      "Processed data for sub_id 028\n",
      "Processed data for sub_id 031\n",
      "Processed data for sub_id 032\n",
      "Processed data for sub_id 033\n",
      "Processed data for sub_id 036\n",
      "Processed data for sub_id 038\n",
      "Processed data for sub_id 041\n",
      "Processed data for sub_id 042\n",
      "Processed data for sub_id 044\n",
      "Processed data for sub_id 045\n",
      "Processed data for sub_id 047\n",
      "Processed data for sub_id 053\n",
      "Processed data for sub_id 054\n",
      "Processed data for sub_id 055\n",
      "Processed data for sub_id 060\n",
      "Processed data for sub_id 063\n",
      "Processed data for sub_id 064\n",
      "Processed data for sub_id 066\n",
      "Processed data for sub_id 070\n",
      "Processed data for sub_id 072\n",
      "Processed data for sub_id 073\n",
      "Processed data for sub_id 075\n",
      "Processed data for sub_id 076\n",
      "Processed data for sub_id 077\n",
      "Processed data for sub_id 078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def load_and_scale_eeg_data(eeg_folder_path, sub_id_path, batch_size=6):\n",
    "    # Load sub_id\n",
    "    sub_id_df = pd.read_csv(sub_id_path)\n",
    "    sub_id_list = sub_id_df['sub_id'].apply(lambda x: f\"{x:03d}\").tolist()\n",
    "\n",
    "    max_length = 0\n",
    "    eeg_data = []\n",
    "    for sub_id in sub_id_list:\n",
    "        mat_file_path = os.path.join(eeg_folder_path, f\"{sub_id}.mat\")\n",
    "        if os.path.exists(mat_file_path):\n",
    "            mat = loadmat(mat_file_path)\n",
    "        else:\n",
    "            print(f\"Error: File not found for sub_id {sub_id}\")\n",
    "            continue\n",
    "\n",
    "        for segment in ['Segment_3', 'Segment_6', 'Segment_9', 'Segment_12', 'Segment_15', 'Segment_18']:\n",
    "            eeg_segment = mat[segment]\n",
    "            max_length = max(max_length, eeg_segment.shape[1])  # update max_length\n",
    "            eeg_data.append(eeg_segment)\n",
    "\n",
    "        print(f\"Processed data for sub_id {sub_id}\")\n",
    "\n",
    "    # Zero padding\n",
    "    for i in range(len(eeg_data)):\n",
    "        if eeg_data[i].shape[1] < max_length:\n",
    "            zero_pad = np.zeros((eeg_data[i].shape[0], max_length - eeg_data[i].shape[1]))\n",
    "            eeg_data[i] = np.hstack((eeg_data[i], zero_pad))\n",
    "\n",
    "    # Convert list of arrays into 3D array\n",
    "    eeg_data = np.stack(eeg_data)\n",
    "\n",
    "    # Min-Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    eeg_data = eeg_data.reshape(-1, eeg_data.shape[-1])  # collapse the first two dimensions\n",
    "    eeg_data = scaler.fit_transform(eeg_data)  # scale the data\n",
    "    eeg_data = eeg_data.reshape(-1, max_length, 10)  # reshape back into original shape\n",
    "\n",
    "    # Split the data into batches\n",
    "    num_batches = eeg_data.shape[0] // batch_size\n",
    "    eeg_data = eeg_data[:num_batches*batch_size]  # ignore last few samples if not enough for a batch\n",
    "    eeg_data = eeg_data.reshape(num_batches, batch_size, max_length, 10)\n",
    "\n",
    "    return eeg_data\n",
    "\n",
    "eeg_data = load_and_scale_eeg_data(eeg_folder_path, sub_id_path, batch_size=6)\n",
    "eeg_data = eeg_data.astype(np.float32)  # change the data type to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d444ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of eeg_data: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of eeg_data:\", eeg_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a0a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that 'eeg_data_scaled' is a list of EEG data arrays\n",
    "X = np.array(eeg_data)  # convert list to numpy array\n",
    "X = X.reshape(-1, *X.shape[2:])  # remove batch dimension and make it 3D\n",
    "\n",
    "# Adding Gaussian Noise\n",
    "noise_factor = 0.5\n",
    "X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f612502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "78008\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "print(X_noisy.dtype)\n",
    "print(X.shape[1])\n",
    "print(X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a915a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 11:03:03.581209: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# 모델 아키텍처에 투입해서 특징 추출\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D\n",
    "\n",
    "# Shape of single EEG data array\n",
    "n_time_steps, n_channels = X.shape[1], X.shape[2]\n",
    "\n",
    "# Input layer\n",
    "eeg_input = Input(shape=(n_time_steps, n_channels), name='eeg_input', dtype=tf.float32)  # specify the data type of Input layer\n",
    "\n",
    "# Encoding\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(eeg_input)\n",
    "x = MaxPooling1D(2, padding='same')(x)\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "# Decoding\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(encoded)\n",
    "x = UpSampling1D(2)(x)\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "x = UpSampling1D(2)(x)\n",
    "decoded = Conv1D(n_channels, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Build the autoencoder model\n",
    "autoencoder = Model(inputs=eeg_input, outputs=decoded)\n",
    "\n",
    "# Build the feature extraction model (encoder part of the autoencoder)\n",
    "feature_extractor = Model(inputs=eeg_input, outputs=encoded)\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder with noisy data as input and clean data as output\n",
    "autoencoder.fit(X_noisy, X, epochs=30, batch_size=16, shuffle=True, verbose=2)\n",
    "\n",
    "# Extract features using the feature_extractor model\n",
    "features = feature_extractor.predict(X_noisy)\n",
    "\n",
    "# Flatten the features\n",
    "features_flattened = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Save the flattened features\n",
    "np.save('features.npy', features_flattened)\n",
    "\n",
    "# Check the shape and content of features\n",
    "print(features_flattened.shape)\n",
    "print(features_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f40b1",
   "metadata": {},
   "source": [
    "# Concatenate (#1 + #2 + #3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf89722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "\n",
    "# Concatenate the output from the different branches\n",
    "concatenated = Concatenate()([sub_id_output, video_id_output, eeg_output])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
