{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6445b2e6",
   "metadata": {},
   "source": [
    "## EEG data grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c73ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 경로 설정\n",
    "eeg_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG_test_csv'\n",
    "\n",
    "# 파일 목록 가져오기\n",
    "file_list = os.listdir(eeg_folder_path)\n",
    "\n",
    "# 그룹별로 파일들을 그룹화\n",
    "grouped_files = {}\n",
    "for file_name in file_list:\n",
    "    group_key = file_name[:11]\n",
    "    if group_key == \".DS_Store\":  # .DS_Store 그룹은 건너뜁니다\n",
    "        continue\n",
    "    if group_key not in grouped_files:\n",
    "        grouped_files[group_key] = []\n",
    "    grouped_files[group_key].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa674b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 데이터를 저장할 리스트\n",
    "new_data = []\n",
    "\n",
    "# 그룹별로 데이터 처리\n",
    "for key, file_names in grouped_files.items():\n",
    "    # 그룹 내 파일들을 읽어와 데이터 리스트에 추가\n",
    "    group_data = []\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(eeg_folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, encoding='latin1')  # 인코딩 변경\n",
    "        channel_data = df.iloc[:, 0].values  # 첫 번째 열의 데이터만 사용\n",
    "        group_data.append(channel_data)\n",
    "    \n",
    "    # 그룹 데이터를 평균하여 새로운 데이터 생성\n",
    "    new_group_data = np.mean(group_data, axis=0)\n",
    "    new_data.append(new_group_data)\n",
    "\n",
    "# 저장할 폴더 경로\n",
    "output_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG_test2_csv'\n",
    "\n",
    "# 데이터를 CSV 파일로 저장\n",
    "for i, group_data in enumerate(new_data):\n",
    "    file_name = f\"group_{i+1}.csv\"\n",
    "    file_path = os.path.join(output_folder_path, file_name)\n",
    "    pd.DataFrame(group_data).to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f82f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그룹 개수: 108\n",
      "002_video_1\n",
      "002_video_2\n",
      "002_video_3\n",
      "002_video_4\n",
      "002_video_5\n",
      "002_video_6\n",
      "003_video_1\n",
      "003_video_2\n",
      "003_video_3\n",
      "003_video_4\n",
      "003_video_5\n",
      "003_video_6\n",
      "007_video_1\n",
      "007_video_2\n",
      "007_video_3\n",
      "007_video_4\n",
      "007_video_5\n",
      "007_video_6\n",
      "017_video_1\n",
      "017_video_2\n",
      "017_video_3\n",
      "017_video_4\n",
      "017_video_5\n",
      "017_video_6\n",
      "018_video_1\n",
      "018_video_2\n",
      "018_video_3\n",
      "018_video_4\n",
      "018_video_5\n",
      "018_video_6\n",
      "019_video_1\n",
      "019_video_2\n",
      "019_video_3\n",
      "019_video_4\n",
      "019_video_5\n",
      "019_video_6\n",
      "020_video_1\n",
      "020_video_2\n",
      "020_video_3\n",
      "020_video_4\n",
      "020_video_5\n",
      "020_video_6\n",
      "025_video_1\n",
      "025_video_2\n",
      "025_video_3\n",
      "025_video_4\n",
      "025_video_5\n",
      "025_video_6\n",
      "029_video_1\n",
      "029_video_2\n",
      "029_video_3\n",
      "029_video_4\n",
      "029_video_5\n",
      "029_video_6\n",
      "030_video_1\n",
      "030_video_2\n",
      "030_video_3\n",
      "030_video_4\n",
      "030_video_5\n",
      "030_video_6\n",
      "039_video_1\n",
      "039_video_2\n",
      "039_video_3\n",
      "039_video_4\n",
      "039_video_5\n",
      "039_video_6\n",
      "043_video_1\n",
      "043_video_2\n",
      "043_video_3\n",
      "043_video_4\n",
      "043_video_5\n",
      "043_video_6\n",
      "048_video_1\n",
      "048_video_2\n",
      "048_video_3\n",
      "048_video_4\n",
      "048_video_5\n",
      "048_video_6\n",
      "052_video_1\n",
      "052_video_2\n",
      "052_video_3\n",
      "052_video_4\n",
      "052_video_5\n",
      "052_video_6\n",
      "059_video_1\n",
      "059_video_2\n",
      "059_video_3\n",
      "059_video_4\n",
      "059_video_5\n",
      "059_video_6\n",
      "065_video_1\n",
      "065_video_2\n",
      "065_video_3\n",
      "065_video_4\n",
      "065_video_5\n",
      "065_video_6\n",
      "068_video_1\n",
      "068_video_2\n",
      "068_video_3\n",
      "068_video_4\n",
      "068_video_5\n",
      "068_video_6\n",
      "074_video_1\n",
      "074_video_2\n",
      "074_video_3\n",
      "074_video_4\n",
      "074_video_5\n",
      "074_video_6\n"
     ]
    }
   ],
   "source": [
    "# 그룹 개수 출력\n",
    "print(f\"그룹 개수: {len(grouped_files)}\")\n",
    "\n",
    "# 그룹 이름 출력 (정렬하여)\n",
    "for group_name in sorted(grouped_files.keys()):\n",
    "    print(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bacd72b",
   "metadata": {},
   "source": [
    "# EEG preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d823d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 경로 설정\n",
    "eeg_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG_test2_csv'\n",
    "\n",
    "# 파일 목록 가져오기\n",
    "file_list = os.listdir(eeg_folder_path)\n",
    "\n",
    "# 전체 데이터를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 가장 긴 데이터 길이를 기준으로 zero-padding\n",
    "max_length = 0\n",
    "\n",
    "# 데이터 읽어오기 및 전처리\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(eeg_folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    channel_data = df.iloc[:, 0].values  # 첫 번째 열의 데이터만 사용\n",
    "    data.append(channel_data)\n",
    "    if len(channel_data) > max_length:\n",
    "        max_length = len(channel_data)\n",
    "\n",
    "# Zero-padding\n",
    "padded_data = pad_sequences(data, maxlen=max_length, padding='post')\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(padded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3115502",
   "metadata": {},
   "source": [
    "# Denoising AutoEncoder(DAE) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9206f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# 가우시안 노이즈 추가\n",
    "def add_gaussian_noise(data, noise_factor):\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985e91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising Autoencoder 모델 생성\n",
    "def create_denoising_autoencoder(input_shape, encoding_dim):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Flatten()(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(np.prod(input_shape), activation='sigmoid')(encoded)\n",
    "    decoded = Reshape(input_shape)(decoded)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97b48bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 12ms/step - loss: 0.0873\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0734\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0550\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0528\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0505\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0492\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0459\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0451\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0441\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0429\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0419\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0409\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0402\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0395\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0388\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.0381\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0375\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0368\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.0363\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0357\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0352\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.0346\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0341\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0336\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0330\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0325\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0321\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0316\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0312\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0308\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0304\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0305\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0299\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0296\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.0291\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.0283\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0285\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0285\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0275\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0274\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0271\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0263\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0264\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0258\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.0260\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0269\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.0268\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0254\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.0243\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.0238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a79ab400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 데이터의 형태\n",
    "input_shape = normalized_data.shape[1:]\n",
    "encoding_dim = 64\n",
    "\n",
    "# 가우시안 노이즈 추가\n",
    "noise_factor = 0.5\n",
    "noisy_data = add_gaussian_noise(normalized_data, noise_factor)\n",
    "\n",
    "# Denoising Autoencoder 모델 생성\n",
    "autoencoder = create_denoising_autoencoder(input_shape, encoding_dim)\n",
    "\n",
    "# 모델 학습\n",
    "autoencoder.fit(noisy_data, normalized_data, epochs=50, batch_size=6, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19fb8f",
   "metadata": {},
   "source": [
    "# feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49bc9710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 85ms/step\n"
     ]
    }
   ],
   "source": [
    "# 특징 추출 (인코더의 출력)\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('dense_1').output)\n",
    "eeg_test_features = encoder.predict(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f351fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(eeg_test_features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc360540",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/eeg_test_features.txt', eeg_test_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
