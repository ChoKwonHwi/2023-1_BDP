{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "train_ids = ['001', '005', '006', '009', '010', '011', '012', '013', '015', '021', '022', '023',\n",
    "             '024', '026', '027', '028', '031', '032', '033', '036', '038', '041', '042', '044',\n",
    "             '045', '047', '053', '054', '055', '060', '062', '063', '064', '066', '070', '072',\n",
    "             '073', '075', '076', '077', '078']\n",
    "test_ids = ['002', '003', '007', '017', '018', '019', '020', '025', '029', '030', '039', '043',\n",
    "            '048', '052', '059', '065', '068', '074']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'disgust', 'fear', 'sad', 'happy', 'anger', 'neutral', 'anger', 'disgust', 'happy', 'sad', 'fear', 'fear', 'happy', 'sad', 'neutral', 'disgust', 'anger', 'neutral', 'anger', 'sad', 'disgust', 'fear', 'happy', 'fear', 'happy', 'sad', 'neutral', 'anger', 'disgust', 'fear', 'sad', 'anger', 'neutral', 'happy', 'disgust', 'happy', 'disgust', 'fear', 'anger', 'sad', 'neutral', 'anger', 'neutral', 'fear', 'sad', 'disgust', 'happy', 'happy', 'neutral', 'anger', 'fear', 'sad', 'disgust', 'anger', 'fear', 'neutral', 'disgust', 'sad', 'happy', 'disgust', 'sad', 'neutral', 'happy', 'fear', 'anger', 'sad', 'fear', 'neutral', 'happy', 'disgust', 'anger', 'fear', 'sad', 'disgust', 'happy', 'neutral', 'anger', 'neutral', 'disgust', 'anger', 'happy', 'sad', 'fear', 'anger', 'happy', 'sad', 'fear', 'disgust', 'neutral', 'fear', 'neutral', 'disgust', 'anger', 'sad', 'happy', 'sad', 'happy', 'disgust', 'anger', 'fear', 'neutral', 'happy', 'neutral', 'disgust', 'sad', 'fear', 'anger', 'happy', 'sad', 'neutral', 'fear', 'disgust', 'anger', 'happy', 'anger', 'sad', 'fear', 'disgust', 'neutral', 'happy', 'neutral', 'fear', 'sad', 'anger', 'disgust', 'anger', 'sad', 'neutral', 'happy', 'disgust', 'fear', 'sad', 'neutral', 'anger', 'fear', 'disgust', 'happy', 'happy', 'neutral', 'anger', 'sad', 'disgust', 'fear', 'neutral', 'sad', 'disgust', 'happy', 'fear', 'anger', 'disgust', 'anger', 'fear', 'neutral', 'happy', 'sad', 'neutral', 'happy', 'sad', 'fear', 'disgust', 'anger', 'fear', 'anger', 'disgust', 'happy', 'neutral', 'sad', 'sad', 'anger', 'neutral', 'disgust', 'happy', 'fear', 'happy', 'anger', 'sad', 'disgust', 'neutral', 'fear', 'happy', 'fear', 'sad', 'neutral', 'disgust', 'anger', 'anger', 'happy', 'fear', 'disgust', 'sad', 'neutral', 'happy', 'anger', 'disgust', 'sad', 'fear', 'neutral', 'neutral', 'disgust', 'happy', 'fear', 'sad', 'anger', 'happy', 'fear', 'anger', 'sad', 'neutral', 'disgust', 'happy', 'fear', 'sad', 'neutral', 'disgust', 'anger', 'disgust', 'anger', 'neutral', 'sad', 'happy', 'fear', 'sad', 'disgust', 'anger', 'neutral', 'fear', 'happy', 'happy', 'sad', 'disgust', 'fear', 'neutral', 'anger', 'fear', 'anger', 'happy', 'sad', 'neutral', 'disgust', 'neutral', 'disgust', 'anger', 'sad', 'happy', 'fear']\n"
     ]
    }
   ],
   "source": [
    "video_id_file = \"video_id.csv\"\n",
    "video_id_df = pd.read_csv(video_id_file)\n",
    "# 두 번째 열을 'emotion' 열로 대체\n",
    "video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "emotions = video_id_df['emotion'].tolist()\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13690\n"
     ]
    }
   ],
   "source": [
    "def get_max_sequence_length(ids):\n",
    "    max_len = 0\n",
    "    for id in ids:\n",
    "        video_id_file = \"video_id.csv\"\n",
    "        video_id_df = pd.read_csv(video_id_file)\n",
    "        video_id_df['video_id'] = video_id_df['video_id'].apply(lambda x: str(x).zfill(3))\n",
    "        video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "        emotions = video_id_df[video_id_df.video_id == id]['emotion'].tolist()\n",
    "        for emotion in emotions:\n",
    "            file_path = f\"{id}/{id}_{emotion}.csv\"\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            sequence_length = len(data)\n",
    "            if sequence_length > max_len:\n",
    "                max_len = sequence_length\n",
    "    return max_len \n",
    "\n",
    "# 최대 시퀀스 길이 확인\n",
    "max_sequence_length = get_max_sequence_length(train_ids)\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(ids, max_len=max_sequence_length):\n",
    "    features = []\n",
    "    scaler = StandardScaler()\n",
    "    for id in ids:\n",
    "        # video_id.csv 파일 가져오기, id 형식 변환\n",
    "        video_id_file = \"video_id.csv\"\n",
    "        video_id_df = pd.read_csv(video_id_file)\n",
    "        video_id_df['video_id'] = video_id_df['video_id'].apply(lambda x: str(x).zfill(3))\n",
    "        # 두 번째 열을 'emotion' 열로 대체\n",
    "        video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "        # 해당 id의 감정 순서 리스트 생성\n",
    "        emotions = video_id_df[video_id_df.video_id == id]['emotion'].tolist()\n",
    "        for emotion in emotions:\n",
    "            # 감정에 해당하는 파일 경로 탐색\n",
    "            file_path = f\"{id}/{id}_{emotion}.csv\"\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            # 데이터 2D 배열로 변환\n",
    "            data = data.values.reshape(-1, 1)\n",
    "            # 데이터 표준화\n",
    "            data = scaler.fit_transform(data)\n",
    "            # 데이터를 max_len으로 잘라냄\n",
    "            data = data[:max_len]\n",
    "            # 만약 데이터의 길이가 max_len보다 작다면, 나머지 부분을 패딩\n",
    "            if len(data) < max_len:\n",
    "                data = np.pad(data, ((0, max_len - len(data)), (0, 0)), 'constant')\n",
    "            # 데이터 추가\n",
    "            features.append(data)\n",
    "\n",
    "    features_padded = np.array(features)\n",
    "    return features_padded\n",
    "\n",
    "train_features = load_and_preprocess_data(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 13690, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.93475448]\n",
      "  [ 2.99404729]\n",
      "  [ 2.96438931]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[-0.50565603]\n",
      "  [-0.51482795]\n",
      "  [-0.51482795]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 2.53794074]\n",
      "  [ 2.57023986]\n",
      "  [ 2.57023986]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.16702695]\n",
      "  [-1.16702695]\n",
      "  [ 0.06997831]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 2.6772027 ]\n",
      "  [ 0.17921862]\n",
      "  [ 0.17921862]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[-0.56540335]\n",
      "  [-1.82864931]\n",
      "  [-0.56540335]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_shape, encoding_dim):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Flatten layer: the Dense layer works on 2D inputs.\n",
    "    flat_layer = Flatten()(input_layer)\n",
    "\n",
    "    # Encoding layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(flat_layer)\n",
    "\n",
    "    # Decoding layer\n",
    "    decoded = Dense(np.prod(input_shape), activation='sigmoid')(encoded)\n",
    "\n",
    "    # Reshape layer: to match the original input shape.\n",
    "    reshaped = Reshape(input_shape)(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, reshaped)\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Encoding dimension\n",
    "encoding_dim = 64  # or any other value\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder, encoder = create_autoencoder((max_sequence_length, 1), encoding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 1s 80ms/step - loss: 0.3945\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3838\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3513\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2999\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2456\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2038\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1788\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1644\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1566\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1512\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1480\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1453\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1435\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1420\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1409\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1400\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1392\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1386\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1380\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1375\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.1371\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1367\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1363\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1359\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1356\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1353\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1350\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1347\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1344\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1341\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1339\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1336\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1334\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1331\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1329\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.1326\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1324\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1321\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1319\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1317\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1314\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1312\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1310\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1307\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1305\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1303\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1301\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1299\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.1297\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1295\n",
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(train_features, train_features, epochs=50, batch_size=128, shuffle=True)\n",
    "\n",
    "# Extracting features\n",
    "train_features_encoded = encoder.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 64)\n"
     ]
    }
   ],
   "source": [
    "print(train_features_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.804008 ,  0.       ,  0.       , ...,  0.       , 21.754387 ,\n",
       "        22.662962 ],\n",
       "       [ 0.       ,  1.4664919, 20.794079 , ...,  7.329501 ,  0.       ,\n",
       "         0.       ],\n",
       "       [22.468359 ,  0.       ,  0.       , ...,  0.       , 24.02456  ,\n",
       "        22.360542 ],\n",
       "       ...,\n",
       "       [ 0.       ,  0.       ,  0.       , ..., 50.487865 ,  0.       ,\n",
       "         0.       ],\n",
       "       [46.718307 ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [10.599931 ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded features to a DataFrame\n",
    "df_encoded = pd.DataFrame(train_features_encoded)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = \"encoded_features_EDA.csv\"\n",
    "df_encoded.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
